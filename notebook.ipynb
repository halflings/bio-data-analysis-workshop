{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Notebook configuration for plotting\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.style.use('ggplot')\n",
    "plt.rcParams['image.cmap'] = 'viridis'\n",
    "plt.rcParams['figure.figsize'] = (18.0, 10.0)\n",
    "\n",
    "COLORS = [(0.8392156862745098, 0.37254901960784315, 0.37254901960784315),\n",
    " (0.2823529411764706, 0.47058823529411764, 0.8117647058823529),\n",
    " (0.41568627450980394, 0.8, 0.396078431372549),\n",
    " (0.7058823529411765, 0.48627450980392156, 0.7803921568627451),\n",
    " (0.7686274509803922, 0.6784313725490196, 0.4),\n",
    " (0.4666666666666667, 0.7450980392156863, 0.8588235294117647)]\n",
    "plt.rcParams['axes.prop_cycle'] = plt.cycler('color', COLORS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Some libraries we'll use in what follows\n",
    "import numpy as np\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# \"Introduction to machine learning\" Workshop\n",
    "\n",
    "Welcome to the machine learning workshop!\n",
    "\n",
    "\n",
    "# What's machine learning ?\n",
    "\n",
    "Machine Learning is about building (statistical) models that improve as they learn from existing data. These models can solve a variety of issues, but the most common are supervised learning problems for **classification** and **prediction** tasks (other tasks include: clustering, generating data, ...)\n",
    "\n",
    "Machine learning can be divided into two main categories: supervised learning and unsupervised learning.\n",
    "\n",
    "### Supervised Learning\n",
    "\n",
    "In **Supervised Learning**, we have a dataset consisting of both features and the expected output values (labels).\n",
    "The task is to train a model that is able to predict the label of an object\n",
    "given its features. For example, predicting if someone has the flu based on physiological measurements.\n",
    "\n",
    "Some more complicated examples are:\n",
    "\n",
    "- given a photograph of a person, identify the person in the photo.\n",
    "- given the list of songs a user listened to, recommend a song they would like\n",
    "  (we call this a *recommender system*, something we're actively working on at Spotify).\n",
    "\n",
    "In the case of **classification**, the labels are discrete (usually strings or a limited number of integers). For example: identifying which species is on a photograph.\n",
    "In the case of **regression**, the labels are continuous (usually floats or vectors of floats). For example: predicting the weight of a person based on her diet.\n",
    "\n",
    "### Unsupervised Learning\n",
    "\n",
    "In **Unsupervised Learning**, there data points are not labeled and the task is usually to detect some fundamental structure present in the data: data points that could be grouped together, correlated dimensions, etc.\n",
    "\n",
    "Examples of **unsupervised learning** tasks are:\n",
    "\n",
    "- clustering politicians based on their voting history\n",
    "- finding topics discussed in the aftonbladet\n",
    "- decomposing an electrical signal into the sub-signals that compose it\n",
    "\n",
    "\n",
    "# What will we do in this workshop?\n",
    "\n",
    "We'll focus on **supervised learning** with a classification task.\n",
    "\n",
    "We're going to use data donated by the University of California, Irvine, about cancer diagnostics.\n",
    "\n",
    "The data consists of 32 columns:\n",
    "\n",
    "1. ID number \n",
    "2. Diagnosis (M = malignant, B = benign) \n",
    "3. (to 32)\n",
    "    Ten real-valued features are computed for each cell nucleus: \n",
    "\n",
    "    - radius (mean of distances from center to points on the perimeter) \n",
    "    - texture (standard deviation of gray-scale values) \n",
    "    - perimeter \n",
    "    - area \n",
    "    - smoothness (local variation in radius lengths) \n",
    "    - compactness (perimeter^2 / area - 1.0) \n",
    "    - concavity (severity of concave portions of the contour) \n",
    "    - concave points (number of concave portions of the contour) \n",
    "    - symmetry \n",
    "    - fractal dimension (\"coastline approximation\" - 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I: Data loading and exploration\n",
    "\n",
    "## * 1) Loading the data\n",
    "\n",
    "We'll try to load the data directly from the UCI website. Theirs servers have been in a pretty bad shape recently, returning a lot of server errors, so we also have a local copy of the data.\n",
    "\n",
    "We'll use `pandas` to load this data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# The dataset doesn't contain a header containing column names\n",
    "# so we generate them ourselves.\n",
    "feature_columns = ['feature_{}'.format(i) for i in range(1, 31)]\n",
    "columns = ['id', 'diagnosis'] + feature_columns\n",
    "\n",
    "# Reading data from a \n",
    "#DATA_PATH = 'https://archive.ics.uci.edu/ml/machine-learning-databases/breast-cancer-wisconsin/wdbc.data'\n",
    "DATA_PATH = 'wdbc.data'\n",
    "df = pd.read_csv(DATA_PATH, header=None, names=columns, index_col='id')\n",
    "df['diagnosis'] = df['diagnosis'].astype('category')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well, that was easier than expected, right?\n",
    "\n",
    "## * 2) Exploring the data\n",
    "\n",
    "Randomly feeding your data into the the newest and most \"hip\" model (_\"RSNMIN: Recurrent Stochastic Neural Machine Implemented in Node.js\"_) is the worst you could do at this point.\n",
    "\n",
    "You should first try to explore your data and have a feel of how it's distributed, if there are any values missing, any special correlations, etc. Only after having all this information will you be able to choose the right model(s) and pre-processing to use.\n",
    "\n",
    "Let's start by looking at how our data is distributed:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "_ = df.hist(bins=15, figsize=(24, 16))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the features seem to follow a gaussian distributions. Perfect! This is usually how you'd like your data to be distributed.\n",
    "\n",
    "Uniformly distributed data and greatly skewed distributions can be painful to deal with, as they might not provide as much information to your models.\n",
    "\n",
    "What we did just now is called **univariate distributions**: how every variable is distributed independantly of the others. Let's try to look at some **multivariate distributions**, and to be more precise: bi-variate distributions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from pandas.tools.plotting import scatter_matrix\n",
    "label_colors = ['b' if d == 'B' else 'r' for d in df['diagnosis']]\n",
    "_ = scatter_matrix(df[['feature_1', 'feature_2', 'feature_3', 'feature_20']], c=label_colors, diagonal='kde', s=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have 3 observations to make here:\n",
    "\n",
    "- Correlated variables: It's pretty obvious that **`feature_1`** and **`feature_3`** are highly correlated.\n",
    "- Like we saw in the previous plot, **`feature_20`** is highly skewed towards lower values, with some outliers in a higher range.\n",
    "- The other variables are not particularly correlated.\n",
    "\n",
    "Correlated variables can have a big impact on the performance of our models. Some models are highly sensitive to them and work best with independant variables (like `Naive Bayes`) while others won't be affected as much by these correlations (like `Logistic Regression`, thanks to the regularization term that will *eliminate* variables that do not contribute anything new).\n",
    "\n",
    "To get a more formal view of how our variables are correlated, we can calculate a \"correlation\" matrix using metrics like the `Pearson correlation`: the higher the absolute value of this metric is, the more the variables are correlated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "correlation_matrix = np.corrcoef(df[feature_columns].values.T)\n",
    "plt.matshow(np.abs(correlation_matrix))\n",
    "plt.title('Pearson correlation matrix')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From looking at these matrices, we can see that a strong correlation exists between some features of the first molecule and the equivalent features in the third one. We can confirm this by extracting strongly correlated features and visualizing them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import itertools\n",
    "strongly_correlated_couples = [(feature_columns[i], feature_columns[j]) for i in range(30) for j in range(i+1, 30)\n",
    "                               if abs(correlation_matrix[i, j]) >= 0.98] \n",
    "strongly_correlated_features = list(set(feature for f_couple in strongly_correlated_couples for feature in f_couple))\n",
    "\n",
    "_ = scatter_matrix(df[strongly_correlated_features], c=label_colors, s=25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II: The *actual* machine learning\n",
    "(aka, building models!)\n",
    "\n",
    "## Model choice and pre-processing\n",
    "\n",
    "\n",
    "Now that we have a better idea of what our data looks like, we can start the modelling part.\n",
    "\n",
    "Depending on what models we use and the distribution of the data, it can be a good idea to do some \"feature engineering\" and do some pre-processing on the features.\n",
    "\n",
    "Some common pre-processing operations:\n",
    "- Normalizing the data: Centering the component around 0 with a standard deviation of 1.\n",
    "- Scaling the data: Making sure all values of the component are comprised between certain minimum and maximum values.\n",
    "- Feature selection: It might be good to drop some features if they have a negative impact on the performance of the model.\n",
    "- Dimensionality reduction: If we have a very high number of features, including many that are highly correlated, it's a good idea to reduce the number of components by keeping the maximum amount of information.\n",
    "\n",
    "Some models perform better if the data is normalized (Support Vector Machines, some Neural Networks), others are sensitive to correlated features (Naive Bayes). Depending on the model you choose, some pre-processing steps might improve or worsen performance. So choose wisely!\n",
    "\n",
    "## Evaluation\n",
    "\n",
    "This is usually seen as the last step you take to check everything is working correctly once you've built your model... but really, it's more of a feedback loop than a sequential process!\n",
    "\n",
    "Many models require **hyper-parameters** which you have to tune based on the performance of your model. It is (sometimes) possible to choose a sensible value for a hyper-parameter based on the nature of the data, but it's most likely that you'll just have to try a large number of values until you find those that work best for your data... and you need to do **model evaluation** for that!\n",
    "\n",
    "## Which library to use?\n",
    "\n",
    "The clear leader in this field is **`scikit-learn`**: this is the most popular machine learning library in Python and is by itself enough of a reason to use Python instead of something like Java. It's under active developement, contains tons of models and tools to do feature engineering / pre-processing / visualize data. It's great!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1) A first simple model\n",
    "\n",
    "It's often good to start with the simplest model [(occam's razor!)](https://en.wikipedia.org/wiki/Occam%27s_razor), so let's do a simple **Logistic Regression**: this model is usually used for binary classification tasks but can be extended for multi-class classifications. **`scikit-learn`** makes it super easy to use a model.\n",
    "\n",
    "Let's first look at the **features** and **labels** we'll use with our model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features = df[feature_columns].values\n",
    "labels = df['diagnosis'].values.to_dense()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "all_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "labels[:6]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Great!\n",
    "\n",
    "Let's now instantiate a **`LogisticRegressionCV`** model that we'll feed our feature vectors and labels to:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "from sklearn.cross_validation import train_test_split\n",
    "\n",
    "all_features = df[feature_columns].values\n",
    "labels = df['diagnosis'].values.to_dense()\n",
    "\n",
    "model = LogisticRegressionCV()\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(all_features, labels, train_size=0.33, random_state=42)\n",
    "\n",
    "model.fit(X_train, y_train)\n",
    "\n",
    "model.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Awesome!\n",
    "\n",
    "What we've done here is:\n",
    "\n",
    "1. Initialize the `LogisticRegressionCV` class (if you want to use a different model, just initialize a different class)\n",
    "2. Split the data into training and test data. This is **very** important: if you train and test your models on the same data, you'll tend to optimize towards an unrealistic case. What you're trying to do is classify data **you've never seen before**.\n",
    "3. We predict the class of the training test.\n",
    "\n",
    "That's cool and all, but how can we know if our model performs well? Well, **`scikit-learn`** has a set of tools specifically dedicated to this task, and they're pretty easy to use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import cross_val_score\n",
    "cross_val_score(model, all_features, labels, cv=8, scoring='accuracy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Every value is the metric we chose (`accuracy` in this case) for every split of the data.\n",
    "\n",
    "It's important to choose the right metric for evaluation, because depending on what you're trying to do you'll want to optimize for different things.\n",
    "\n",
    "For instance, maybe it's OK to make a False Positive (predicting a `benign` cancer as being `malignant`), but it's super dangerous to do False Negatives (predicting a `malignant` cancer as being `benign`). And maybe you're doing fraud detection and you want to minimize the number of False Positives because every one of them costs you a lot of money to investigate.\n",
    "\n",
    "Now that we know how to evaluate a model, let's try to look at something a bit more complex: how to pick the right variables for our model?\n",
    "\n",
    "## 2) Feature selection\n",
    "\n",
    "There's many ways to do feature selection: based on the variance of the data, based on its contribution to the model, etc.\n",
    "\n",
    "We'll use \"recursive feature elimination\" to see pick the right features for our model, this time a **Support Vector Machine**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "from sklearn.feature_selection import RFECV\n",
    "from sklearn.cross_validation import StratifiedKFold\n",
    "\n",
    "svc = SVC(kernel=\"linear\", C=1)\n",
    "rfecv = RFECV(estimator=svc, step=1, cv=StratifiedKFold(labels, 2),\n",
    "              scoring='accuracy')\n",
    "rfecv.fit(all_features, labels)\n",
    "\n",
    "print(\"Optimal number of features : %d\" % rfecv.n_features_)\n",
    "\n",
    "# Plot number of features VS. cross-validation scores\n",
    "plt.figure()\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (nb of correct classifications)\")\n",
    "plt.plot(range(1, len(rfecv.grid_scores_) + 1), rfecv.grid_scores_)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty easy, right?\n",
    "\n",
    "It can be surprising that **more features might be equal to worse performance**. There's many possible reasons, but one might be that those features are too noisy and cause the model to **over-fit**: fit specific training data-points instead of generalizing to be usable on any data-point.\n",
    "\n",
    "## 3) Dimensionality reduction\n",
    "\n",
    "Some models work best when features are uncorrelated, and sometimes you just have too many features and training your model takes too much time.\n",
    "\n",
    "For both these cases, **dimensionality reduction** can be useful. This is how we call all methods used to generate a new, smaller, set of features.\n",
    "\n",
    "One of the most popular methods is **PCA** (Principal Component Analysis) and it uses the covariance of your variables to build a new vector space (with generally less components than the original space) where all dimensions are independant, and where feature vectors can be projected by losing the minimum amount of information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.linear_model import LogisticRegressionCV\n",
    "\n",
    "# PCA projections\n",
    "pca = PCA(n_components=10)\n",
    "proj_features = pca.fit_transform(all_features)\n",
    "\n",
    "model = LogisticRegressionCV()\n",
    "print(\"Accuracy using all ({}) features:\".format(all_features.shape[1]))\n",
    "print(cross_val_score(model, all_features, labels, cv=5).mean())\n",
    "\n",
    "print(\"Accuracy using only {} features:\".format(proj_features.shape[1]))\n",
    "print(cross_val_score(model, proj_features, labels, cv=5).mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## But wait... what about regression?\n",
    "\n",
    "All what we described above works more or less the same way for regression problems!\n",
    "\n",
    "Let's try to find the value of two variables based on the remaining variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "regr_features = ['feature_1', 'feature_3']\n",
    "other_features = [f for f in feature_columns if not f in regr_feature]\n",
    "\n",
    "model = LinearRegression()\n",
    "\n",
    "cross_val_score(model, df[other_features], df[regr_feature], cv=8, scoring='mean_squared_error').mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# That's about it!\n",
    "\n",
    "### Things this notebook doesn't talk about:\n",
    "\n",
    "- Unsupervised learning\n",
    "- Specificities of regression vs classification\n",
    "- Specificities of every model\n",
    "- Families of models (linear models, ensemble models, etc.)\n",
    "- More advanced feature engineering \"tricks\"\n",
    "- ...\n",
    "\n",
    "## Let's talk about them now! Time for questions.\n",
    "\n",
    "- - - - - - - - - - "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Annex (fresh out the oven!) - Pandas for protein data (PDB files)\n",
    "\n",
    "Just last week, [Sebastian Raschka](http://github.com/rasbt) released a Python library made for bioscientists called **biopandas**. This library lets you easily load protein data stored in the popular PDB (Protein Data Bank) format."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Initialize a new PandasPDB object\n",
    "# and fetch the PDB file from rcsb.org\n",
    "from biopandas.pdb import PandasPDB\n",
    "ppdb = PandasPDB().fetch_pdb('3eiy')\n",
    "ppdb.df['ATOM'].head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
